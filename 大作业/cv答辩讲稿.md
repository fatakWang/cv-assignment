图像拼接

## 图像拼接定义

图像拼接将数张有重叠部分的图像（可能是不同时间、不同视角或者不同传感器获得的）拼成一幅无缝的全景图。

![00005](D:\study\UnsupDIS-pytorch-main\owndata\test\input2\00005.jpg)![00005](D:\study\UnsupDIS-pytorch-main\owndata\test\input1\00005.jpg)

![00005](D:\study\UnsupDIS-pytorch-main\owndata\sift\00005.jpg)

## 传统图像拼接步骤

以两张图片为例，应用最广泛的传统图像拼接主要分为如下步骤，

1. 在两张图片上面提取特征点，在特征点周围提取特征描述符
2. 两张图片的特征描述符做一一匹配，建立两张图片同一位置的匹配关系
3. 以其中一张图片作为参考图像、另一张图像作为待变换图像，利用第2步计算所得的位置匹配关系计算从待变换图像变换到参考图像所在平面的单应变换矩阵
4. 对待变换图像做单应变换完成图像拼接

## 算法细节

1. 在两张图片上面提取特征点，在特征点周围提取特征描述符

一般就是使用sift来提取特征点，当然用其他的角点检测算法也是可以的，比如说harris 角点检测。特征描述符的话，最流行的也就是sift这牌你论文提出的的特征描述符，这是一个128维的特征向量。而这一个操作在我们opencv库中就一行代码的事情。

```python
descriptor = cv2.xfeatures2d.SIFT_create()
descriptor.detectAndCompute(image, None)
```

当然我们手写完成了上述过程，使用的特征点提取方法是harris角点检测算法，对Harris的响应矩阵做了非极大值抑制后得到了特征点。使用的特征描述符是hog特征描述符。hog特征描述符可以看作sift特征描述符的青春mini版，它计算的是特征点周围的梯度直方图。





它是这样一个算法。在前面计算得到的特征点周围选取一定长宽的小切片，对于这个patch，横竖切若干刀得到若干个网格，这些网格长宽一般就是8个像素，然后对每个网格执行相同的操作，这个操作就是计算x方向、y方向上的梯度，由这二者可以得到图像梯度的方向和梯度大小的绝对值，然后将梯度方向离散成9个标量（就是对梯度方向做了一个直方图统计，0-45°的归为一类，一次类推），就计算每个网格内的这么一个直方图，然后将所有网格的梯度直方图拼接直接展开就能够描述这个小切片。

2. 建立匹配关系

有了特征点及其描述符，我们就可以去做匹配了，最常用的匹配就是一个knn，比如从左边这次图像得到的特征描述符是A，从右边这张图得到的特征描述符是B，对A中的每一个特征描述符都去计算这个特征描述符与B中每一个特征描述符的距离，然后得到前k个距离最小的特征描述符，在这里我们k取2，也就是得到距离最近和次近的两个距离，如果最近/次近小于给定的一定阈值，就认为这两个特征描述符是匹配到一起的。

上述算法可以调库实现

```
matcher = cv2.BFMatcher()
rawMatches = matcher.knnMatch(features_right, features_left, 2)
```

当然也可以自己手写，就以A为行，B为列，计算一个距离矩阵，在对矩阵每一行排序，取前两个就行了。

3. 计算空间变换矩阵

首先介绍一下单应矩阵，单应矩阵是一个有八个自由度的空间变换模型，微观上看它对点做变换，可以把点从一个位置变换到另一个位置。宏观上看，它可以对图像完成平移、缩放、旋转、仿射、投影变换，这些变换全都融合在一个3*3的矩阵里面了。然后因为它最后一个元素是一个比例因子，一般取1，所以自由度就是3\*3-1=8个自由度。然后将点写作齐次坐标，
$$
A=H^{-1}B
$$


也就是Ha=b，这里ab都是一个点，具体到图像拼接，a就是待拼接图像上的点，b就是参考图像上的点。这样的一个对应关系可以列2个方程组，所以理论上4个方程组就可以求解出单应矩阵了。而前一步匹配关系肯定不只4个。其中有大量干扰点。可行不好用的方法是最小二乘法，因为这些干扰点会严重影响计算出来的矩阵。一个好用也是最常用的方式就是RANSAC算法，它去排除干扰点的影响，强化正确点的影响。

在opencv里面，这也是一行代码搞定的事情。不过我们也是手写了的。这里篇幅有限就不介绍这个算法了。

```
(H, status) = cv2.findHomography(pts_right, pts_left, cv2.RANSAC, reprojThresh)
```

4. 对图像做单应变换

前面说过只需要对待拼接点全部做变换就可以得到变换后的图像了，Ha=b，a就是每一个带破解的点，b就是变换后的点的坐标。但是这样不好，因为H左乘了A后得到的b往往是一个浮点数，但是我们的图像都是离散的，当然可以对浮点数做一个四舍五入，但是这样难免影响图像质量或者形成空洞效应。所以一个更加合理的方法是对待拼接的位置左乘一个h的逆矩阵，得到一个待拼接位置的采样坐标，再对待拼接图像做一个双线性插值。再opencv里面这也是一行代码就可以搞定的事情，不过我们也手写了。

```python
result = cv2.warpPerspective(image_tar, H, (image_tar.shape[1] + image_ref.shape[1], image_tar.shape[0]))

```

经过上述四个步骤我们就成功的完成了图像拼接。

## 深度学习图像拼接步骤

不过这些都是传统的方法，我们也调研了深度学习方法是怎么处理这个问题的，现在图像拼接领域的sota是发表在

IEEE Transaction on Image processing（TIP）期刊上面的论文，这篇期刊影响因子11.04是sci 一区期刊也是ccfA类期刊，也就是所谓的顶刊。我之所以前面不停地强调可微就是因为，可微就意味着可以反向传播，可以反向传播就意味着可以被训练。这篇论文将图像拼接这个任务拆分为了图像对齐和图像融合两个pipeline，图像对齐大致就是上面四个步骤，图像融合在图像对齐的基础上解决了来自空间单应变换的不足，比如单应变换这个模型是针对拍摄物体离摄像机无穷远或者距离相同的理想情况的，但是现实就是自然图像肯定离摄像头距离有远有近，这样去计算单应矩阵并变换后会有鬼影出现，图像融合就是去处理这些鬼影的。

 ![image-20230411203822311](C:\Users\fatak\AppData\Roaming\Typora\typora-user-images\image-20230411203822311.png)

我们讲一讲这个图像对齐的pipeline，参考图像和待变换的图像，利用卷积操作提取特征，同时经过池化，来得到这个特征的金字塔，然后从最右边也就是池化下采样了八倍的特征开始预测待变换图像到参考图像的单应矩阵，这个首先是经过了correlation layer，其实就跟前面的特征匹配有一分相像，这个correlation layer是深度学习运用到光流估计的一个模块，可微分的计算两个特征图之间的相似程度，correlation layer处理后经过一个全连接层回归到待变换图像到参考图像的偏移，然后这个偏移在经过我先前说的可微分的svd来求解出单应矩阵。在经过我说过的可微分的双线性插值把最右边这个target特征图变换到reference 特征图上，这是第一次预测。然后前面这两次都是以上一次预测偏移的结果来由粗到细的来预测偏移，逐步的修正这个预测结果，最后得到变换后的图像。它的loss function是一个无监督的，由reference图像与target图像重叠区域的L1损失来计算一个loss，这个L1损失就意味着，变换后的target与reference越相似，loss越小，越不相识loss就越大。然后右loss去计算一个梯度在经过一个方向传播整个网络就训练起来了。

至于图像融合，它网络的架构就是unet形式的一个网络，它的创新之处在于提出了三个loss function来训练这个网络，把前面这个图像对齐得到的结果来融合起来。篇幅有限就不介绍了。

![image-20230411205130339](C:\Users\fatak\AppData\Roaming\Typora\typora-user-images\image-20230411205130339.png)

### 复现结果





前面我们得到了描述将待拼接图像变换到参考图像的单应矩阵，现在我们只需要对所有待拼接图像上的像素点做个空间变换就解决问题了，但是这样不好，因为H左乘了A后得到的坐标往往是一个浮点数，但是我们的图像都是离散的，当然可以对浮点数做一个四舍五入，但是这样难免影响图像质量或者形成空洞效应。所以一个更加合理的方法是对待拼接的位置左乘一个h的逆矩阵，得到一组采样坐标，在根据这个坐标去A中选取对应的像素，这个采样过程实际上可以使用双线性插值，虽然这个采样坐标也是浮点数，就像图中的情况，但是双线性插值干的事情就是利用周边四个点的像素去估计他的像素，这样得到的图像就更加平滑也可以避免这个孔洞效应。在opencv里面这也是一行代码就可以搞定的事情，不过我们也手写了。其实还有一个你无法拒绝的理由就是双线性插值是一个可以微分的过程。而前面这个四舍五入的过程是不可以微分的



为了避免空洞效应和图像质量损失，对于待拼接图像中的像素点，可以通过左乘单应矩阵的逆矩阵得到采样坐标，再利用双线性插值方法去A中选取对应的像素。双线性插值利用周边四个像素点去估计像素值，能够得到更加平滑的图像，同时这个过程也是可微分的。相比之下，使用四舍五入的方法会导致图像质量降低，同时也无法进行微分处理。在opencv中可以用一行代码实现双线性插值的操作。



图像拼接将数张有重叠部分的图像（可能是不同时间、不同视角或者不同传感器获得的）拼成一幅无缝的全景图。

我们从第一步开始，提取特征点，特征描述符

可以使用harris 或者sift来提取特征点。有了特征点就可以在特征点周围取一个小切片来计算特征描述符，像sift那篇论文的话生成的是一个128维的特征描述符。这些操作在opencv库里面就是一行代码的事情。当然我们手写完成了上述操作，手写版本的话特征点提取用的是harris角点检测，特征描述符用的是hog梯度直方图。

第二步特征匹配

有了特征点及其描述符，我们就可以去做匹配了，最常用的匹配就是先做一个knn，这里k取2，得到两组描述符的最近距离，次近距离，如果最近距离除以次近距离小于一个给定的阈值，那么认为这对描述符也就是这对特征是匹配的，这个调库实现的话也是一行代码，不过我们也是手写实现了这个算法,这就是伪代码。然后绿色的线表示匹配关系。

第三步计算单应矩阵

这一步实际上是求解一个有8个自由度的3*3矩阵，这个矩阵可以对图像完成平移、缩放、旋转、仿射、投影变换。参考图像与待变换图像每一对匹配点齐次坐标之间的匹配关系在数学上由这么一个叉乘表示，可以得到一个秩为2的方程组，所以解出单应矩阵需要至少4组匹配点。为了摆脱干扰点的影响，我们使用了RANSAC算法，强化真正匹配点的影响来求解单应矩阵。这些事情opencv调库一行搞定，不过我们也手写了最小二乘法求解单应矩阵和RANSAC算法。

第四步对图像完成单应变换

前面我们得到了描述将待拼接图像变换到参考图像的单应矩阵，现在我们只需要对所有待拼接图像上的像素点做个空间变换就解决问题了，但是这样不好，因为H左乘了A后得到的坐标往往是一个浮点数，但是我们的图像都是离散的，当然可以对浮点数做一个四舍五入，但是这样难免影响图像质量或者形成空洞效应。所以一个更加合理的方法是对待拼接的位置左乘h的逆矩阵，得到一组采样坐标，在根据这个坐标去A中选取对应的像素，这个采样过程实际上可以使用双线性插值，虽然这个采样坐标也是浮点数，就像图中的情况，但是双线性插值干的事情就是利用周边四个点的像素去估计他的像素，这样得到的图像就更加平滑也可以避免这个孔洞效应。在opencv里面这也是一行代码就可以搞定的事情，不过我们也手写了。

这是更多的一些例子

这是发表在2021TIP期刊上的论文，影响因子11、sci一区，ccfA类期刊。这篇论文将图像拼接这个任务拆分为了图像对齐和图像融合两个pipeline，时间有限我就仅介绍图像对齐的pipeline，对reference和target用卷积提取特征，同时经过池化下采样，得到特征金字塔，通过correlation layer可微的在两个特征图间计算相似度并通过全连接层回归到target到refernce的偏移，在可微分的由四个顶点的偏移计算得到单应矩阵，在可微分的做双线性插值将target做空间变换，这个过程在三个下采样倍率不同的特征图上进行来由粗到细的预测修正最终的单应矩阵，最后loss function是reference和target在重叠区域的L1 loss，变换后的target与reference越相似，loss越小，越不相识loss就越大然后计算loss得到梯度，梯度经过这些所有可微分的模块反向传播，整个网络就训练起来了。

这是他比前面传统图像拼接work的一个例子，我们知道sift提取特征的能力是人类赋予的，虽然已经很强了，但有上限，卷积神经网络提取特征的能力是数据赋予的几乎没有上限，对于左侧的两张图，用传统的sift来跑的话，程序会报错因为它甚至找不出四个匹配在一起的特征描述符，sift提取不了这张图片的特征点，但是我们的这篇sota它work了，它拼接出的效果就是右边这张图片